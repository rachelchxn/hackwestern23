{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 1)) (4.8.1.78)\n",
      "Requirement already satisfied: face_recognition in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: openai in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 3)) (1.3.5)\n",
      "Requirement already satisfied: SpeechRecognition in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 4)) (3.10.0)\n",
      "Collecting pyaudio\n",
      "  Using cached PyAudio-0.2.14.tar.gz (47 kB)\n",
      "\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/q4/w5wd298x2j51mk7bkv38kmt80000gn/T/pip-build-env-e4onap9x/normal/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/q4/w5wd298x2j51mk7bkv38kmt80000gn/T/pip-build-env-e4onap9x/normal/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/private/var/folders/q4/w5wd298x2j51mk7bkv38kmt80000gn/T/pip-build-env-e4onap9x/normal'\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/q4/w5wd298x2j51mk7bkv38kmt80000gn/T/pip-build-env-e4onap9x/overlay/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/q4/w5wd298x2j51mk7bkv38kmt80000gn/T/pip-build-env-e4onap9x/overlay/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/private/var/folders/q4/w5wd298x2j51mk7bkv38kmt80000gn/T/pip-build-env-e4onap9x/overlay'\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from opencv-python->-r requirements.txt (line 1)) (1.26.2)\n",
      "Requirement already satisfied: face-recognition-models>=0.3.0 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from face_recognition->-r requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: Pillow in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from face_recognition->-r requirements.txt (line 2)) (10.1.0)\n",
      "Requirement already satisfied: Click>=6.0 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from face_recognition->-r requirements.txt (line 2)) (8.1.7)\n",
      "Requirement already satisfied: dlib>=19.7 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from face_recognition->-r requirements.txt (line 2)) (19.24.2)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from openai->-r requirements.txt (line 3)) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from openai->-r requirements.txt (line 3)) (1.8.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from openai->-r requirements.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from openai->-r requirements.txt (line 3)) (2.5.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from openai->-r requirements.txt (line 3)) (0.25.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from openai->-r requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from SpeechRecognition->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from anyio<4,>=3.5.0->openai->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from anyio<4,>=3.5.0->openai->-r requirements.txt (line 3)) (3.5)\n",
      "Requirement already satisfied: exceptiongroup in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from anyio<4,>=3.5.0->openai->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 3)) (1.0.2)\n",
      "Requirement already satisfied: certifi in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 3)) (2023.11.17)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 3)) (2.14.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->SpeechRecognition->-r requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rachelchen/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->SpeechRecognition->-r requirements.txt (line 4)) (2.1.0)\n",
      "Building wheels for collected packages: pyaudio\n",
      "  Building wheel for pyaudio (PEP 517) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /Library/Developer/CommandLineTools/usr/bin/python3 /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /var/folders/q4/w5wd298x2j51mk7bkv38kmt80000gn/T/tmpl6217l08\n",
      "       cwd: /private/var/folders/q4/w5wd298x2j51mk7bkv38kmt80000gn/T/pip-install-vyo6yqkr/pyaudio_52d6bb9aabd34059a32b2ddafef76ddb\n",
      "  Complete output (17 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.macosx-10.9-universal2-3.9\n",
      "  creating build/lib.macosx-10.9-universal2-3.9/pyaudio\n",
      "  copying src/pyaudio/__init__.py -> build/lib.macosx-10.9-universal2-3.9/pyaudio\n",
      "  running build_ext\n",
      "  creating build/temp.macosx-10.9-universal2-3.9\n",
      "  creating build/temp.macosx-10.9-universal2-3.9/src\n",
      "  creating build/temp.macosx-10.9-universal2-3.9/src/pyaudio\n",
      "  clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/Headers -arch arm64 -arch x86_64 -Werror=implicit-function-declaration -Wno-error=unreachable-code -DMACOS=1 -I/usr/local/include -I/usr/include -I/opt/homebrew/include -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/include/python3.9 -c src/pyaudio/device_api.c -o build/temp.macosx-10.9-universal2-3.9/src/pyaudio/device_api.o\n",
      "  src/pyaudio/device_api.c:9:10: fatal error: 'portaudio.h' file not found\n",
      "  #include \"portaudio.h\"\n",
      "           ^~~~~~~~~~~~~\n",
      "  1 error generated.\n",
      "  error: command '/usr/bin/clang' failed with exit code 1\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\n",
      "Failed to build pyaudio\n",
      "\u001b[31mERROR: Could not build wheels for pyaudio which use PEP 517 and cannot be installed directly\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelchen/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb Cell 3\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m xander_encoding \u001b[39m=\u001b[39m face_recognition\u001b[39m.\u001b[39mface_encodings(xander_image)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m james_image \u001b[39m=\u001b[39m face_recognition\u001b[39m.\u001b[39mload_image_file(\u001b[39m\"\u001b[39m\u001b[39mjames.jpg\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m james_encoding \u001b[39m=\u001b[39m face_recognition\u001b[39m.\u001b[39;49mface_encodings(james_image)[\u001b[39m0\u001b[39;49m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "rachel_image = face_recognition.load_image_file(\"rachel.jpg\")\n",
    "rachel_encoding = face_recognition.face_encodings(rachel_image)[0]\n",
    "\n",
    "xander_image = face_recognition.load_image_file(\"xander.jpg\")\n",
    "xander_encoding = face_recognition.face_encodings(xander_image)[0]\n",
    "\n",
    "james_image = face_recognition.load_image_file(\"james.jpg\")\n",
    "james_encoding = face_recognition.face_encodings(james_image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_encodings = [\n",
    "    rachel_encoding,\n",
    "    xander_encoding\n",
    "]\n",
    "known_face_names = [\n",
    "    \"Rachel Chen\",\n",
    "    \"Xander Chin\"\n",
    "]\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video_capture = cv2.VideoCapture(1)\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Convert the image from BGR color to RGB color\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = np.ascontiguousarray(small_frame[:, :, ::-1])\n",
    "\n",
    "    # Find all the faces and face encodings in the frame of video\n",
    "    face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "    face_names = []\n",
    "    for face_encoding in face_encodings:\n",
    "        # Check if the face is a match for the known face(s)\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=0.5)\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # Use the known face with the smallest distance to the new face\n",
    "        face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "        best_match_index = np.argmin(face_distances)\n",
    "        if matches[best_match_index]:\n",
    "            name = known_face_names[best_match_index]\n",
    "        else:\n",
    "            # If the face is not recognized, add it to the known faces list\n",
    "            known_face_encodings.append(face_encoding)\n",
    "            name = f\"Unnamed Person\"\n",
    "            known_face_names.append(name)\n",
    "\n",
    "        face_names.append(name)\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_transcription():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Say something!\")\n",
    "        while True:\n",
    "            audio = recognizer.listen(source)\n",
    "            try:\n",
    "                # Transcribe the audio to text\n",
    "                text = recognizer.recognize_google(audio)\n",
    "                print(\"Transcription: \" + text)\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Could not understand audio\")\n",
    "            except sr.RequestError as e:\n",
    "                print(\"Error; {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Could not find PyAudio; check installation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/speech_recognition/__init__.py:108\u001b[0m, in \u001b[0;36mMicrophone.get_pyaudio\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpyaudio\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyaudio'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m camera_thread\u001b[39m.\u001b[39mstart()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Running audio transcription in the main thread\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m audio_transcription()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Wait for the camera thread to finish\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m camera_thread\u001b[39m.\u001b[39mjoin()\n",
      "\u001b[1;32m/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maudio_transcription\u001b[39m():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     recognizer \u001b[39m=\u001b[39m sr\u001b[39m.\u001b[39mRecognizer()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mwith\u001b[39;00m sr\u001b[39m.\u001b[39;49mMicrophone() \u001b[39mas\u001b[39;00m source:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSay something!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/speech_recognition/__init__.py:80\u001b[0m, in \u001b[0;36mMicrophone.__init__\u001b[0;34m(self, device_index, sample_rate, chunk_size)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(chunk_size, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m chunk_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mChunk size must be a positive integer\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[39m# set up PyAudio\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyaudio_module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_pyaudio()\n\u001b[1;32m     81\u001b[0m audio \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyaudio_module\u001b[39m.\u001b[39mPyAudio()\n\u001b[1;32m     82\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/speech_recognition/__init__.py:110\u001b[0m, in \u001b[0;36mMicrophone.get_pyaudio\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpyaudio\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not find PyAudio; check installation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdistutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m LooseVersion\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m LooseVersion(pyaudio\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m LooseVersion(\u001b[39m\"\u001b[39m\u001b[39m0.2.11\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: Could not find PyAudio; check installation"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/q4/w5wd298x2j51mk7bkv38kmt80000gn/T/ipykernel_11648/1519530862.py\", line 55, in camera_operations\n",
      "cv2.error: Unknown C++ exception from OpenCV code\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Running camera operations in a separate thread\n",
    "camera_thread = threading.Thread(target=camera_operations)\n",
    "camera_thread.start()\n",
    "\n",
    "# Running audio transcription in the main thread\n",
    "audio_transcription()\n",
    "\n",
    "# Wait for the camera thread to finish\n",
    "camera_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAI\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m client \u001b[39m=\u001b[39m OpenAI()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m audio_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mspeech.mp3\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m transcript \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39maudio\u001b[39m.\u001b[39mtranscriptions\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwhisper-1\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   file\u001b[39m=\u001b[39maudio_file\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_client.py:93\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     91\u001b[0m     api_key \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m api_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m api_key\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m organization \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file = open(\"speech.mp3\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
