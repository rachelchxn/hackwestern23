{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
<<<<<<< Updated upstream
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Invalid requirement: '<<<<<<< Updated upstream' (from line 6 of requirements.txt)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "#!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import threading"
   ]
  },
  {
<<<<<<< Updated upstream
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rachel_image = face_recognition.load_image_file(\"rachel.jpg\")\n",
    "# rachel_encoding = face_recognition.face_encodings(rachel_image)[0]\n",
    "\n",
    "# xander_image = face_recognition.load_image_file(\"xander.jpg\")\n",
    "# xander_encoding = face_recognition.face_encodings(xander_image)[0]\n",
    "\n",
    "# james_image = face_recognition.load_image_file(\"james.jpg\")\n",
    "# james_encoding = face_recognition.face_encodings(james_image)[0]"
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up firebase"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "# known_face_encodings = [\n",
    "#     rachel_encoding,\n",
    "#     xander_encoding\n",
    "# ]\n",
    "# known_face_names = [\n",
    "#     \"Rachel Chen\",\n",
    "#     \"Xander Chin\"\n",
    "# ]\n",
    "\n",
    "# # Initialize some variables\n",
    "# face_locations = []\n",
    "# face_encodings = []\n",
    "# face_names = []\n",
    "# process_this_frame = True\n"
=======
    "from firebase_admin import credentials, firestore, storage\n",
    "import firebase_admin\n",
    "from google.cloud import storage\n",
    "\n",
    "# cred = firebase_admin.credentials.Certificate('hw23-e0512-firebase-adminsdk-3ax9k-293086f6f4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.firebase import uploadImageFromBlob, uploadImageFromPath"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<firebase_admin.App at 0x1d6f184fe80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import firebase_admin\n",
    "\n",
    "cred = firebase_admin.credentials.Certificate('hw23-e0512-firebase-adminsdk-3ax9k-293086f6f4.json')\n",
    "firebase_admin.initialize_app(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firebase_admin import firestore\n",
    "\n",
    "# Get a reference to the Firestore service\n",
    "db = firestore.client()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 7,
=======
   "execution_count": 18,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "#bluetooth stuff\n",
    "\n",
    "import serial\n",
    "from serial import Serial\n",
    "\n",
    "# Define the COM port and baud rate\n",
    "com_port = 'COM9'  # Replace with the actual COM port on your system\n",
    "baud_rate = 9600\n",
    "\n",
    "# Create a serial object\n",
    "ser = serial.Serial(com_port, baud_rate, timeout=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera is off\n",
      "camera is on\n",
      "camera is off\n",
      "camera is on\n",
      "camera is off\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\xande\\Dropbox\\MicrosoftVisualStudioCode\\hackwestern23\\test.ipynb Cell 8\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xande/Dropbox/MicrosoftVisualStudioCode/hackwestern23/test.ipynb#Y125sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m video_capture \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoCapture(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xande/Dropbox/MicrosoftVisualStudioCode/hackwestern23/test.ipynb#Y125sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/xande/Dropbox/MicrosoftVisualStudioCode/hackwestern23/test.ipynb#Y125sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     res \u001b[39m=\u001b[39m ser\u001b[39m.\u001b[39;49mreadline()\u001b[39m.\u001b[39mdecode()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xande/Dropbox/MicrosoftVisualStudioCode/hackwestern23/test.ipynb#Y125sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     \u001b[39m# print(res)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xande/Dropbox/MicrosoftVisualStudioCode/hackwestern23/test.ipynb#Y125sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     \u001b[39mif\u001b[39;00m(res \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\xande\\miniconda3\\envs\\tf\\lib\\site-packages\\serial\\serialwin32.py:288\u001b[0m, in \u001b[0;36mSerial.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m read_ok \u001b[39mand\u001b[39;00m win32\u001b[39m.\u001b[39mGetLastError() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (win32\u001b[39m.\u001b[39mERROR_SUCCESS, win32\u001b[39m.\u001b[39mERROR_IO_PENDING):\n\u001b[0;32m    287\u001b[0m     \u001b[39mraise\u001b[39;00m SerialException(\u001b[39m\"\u001b[39m\u001b[39mReadFile failed (\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(ctypes\u001b[39m.\u001b[39mWinError()))\n\u001b[1;32m--> 288\u001b[0m result_ok \u001b[39m=\u001b[39m win32\u001b[39m.\u001b[39;49mGetOverlappedResult(\n\u001b[0;32m    289\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_port_handle,\n\u001b[0;32m    290\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_overlapped_read),\n\u001b[0;32m    291\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(rc),\n\u001b[0;32m    292\u001b[0m     \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result_ok:\n\u001b[0;32m    294\u001b[0m     \u001b[39mif\u001b[39;00m win32\u001b[39m.\u001b[39mGetLastError() \u001b[39m!=\u001b[39m win32\u001b[39m.\u001b[39mERROR_OPERATION_ABORTED:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def fetch_encodings_from_firestore():\n",
    "    fetched_encodings = []\n",
    "    fetched_names = []\n",
    "    \n",
    "    # Fetch data from Firestore\n",
    "    docs = db.collection('people').stream()\n",
    "    for doc in docs:\n",
    "        data = doc.to_dict()\n",
    "        if 'image_enc' in data and 'name' in data:\n",
    "            fetched_encodings.append(np.array(data['image_enc']))\n",
    "            fetched_names.append(data['name'])\n",
    "    \n",
    "    return fetched_encodings, fetched_names\n",
    "\n",
    "def find_similar_face_key(face_encoding, faces_dict, tolerance=0.6):\n",
    "    for face_key in faces_dict.keys():\n",
    "        if np.linalg.norm(np.array(face_key) - face_encoding) < tolerance:\n",
    "            return face_key\n",
    "    return None\n",
    "\n",
    "\n",
    "# Dictionary to track unrecognized faces\n",
    "unrecognized_faces = {}\n",
    "unrecognized_threshold = 2  # Number of frames to confirm an unrecognized face\n",
    "\n",
<<<<<<< Updated upstream
    "#video_capture = cv2.VideoCapture(0)\n",
=======
    "video_capture = cv2.VideoCapture(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> Stashed changes
    "\n",
    "face_encodings_from_db, face_names_from_db = fetch_encodings_from_firestore()\n",
    "\n",
    "def camera_operations(video_capture, face_encodings_from_db, face_names_from_db):\n",
    "    ret, frame = video_capture.read()\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "    rgb_small_frame = np.ascontiguousarray(small_frame[:, :, ::-1])\n",
    "\n",
    "    face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "    face_names = []\n",
    "    for face_encoding in face_encodings:\n",
    "        matches = face_recognition.compare_faces(face_encodings_from_db, face_encoding, tolerance=0.5)\n",
    "\n",
    "        if True in matches:\n",
    "            first_match_index = matches.index(True)\n",
    "            name = face_names_from_db[first_match_index]\n",
    "            similar_key = find_similar_face_key(face_encoding, unrecognized_faces)\n",
    "            if similar_key is not None:\n",
    "                del unrecognized_faces[similar_key]  # Remove from unrecognized as it's now recognized\n",
    "        else:\n",
    "            similar_key = find_similar_face_key(face_encoding, unrecognized_faces)\n",
    "\n",
    "            if similar_key is not None:\n",
    "                unrecognized_faces[similar_key]['counter'] += 1\n",
    "                print(f\"Counter for a face: {unrecognized_faces[similar_key]['counter']}\")\n",
    "            else:\n",
    "                print(\"New face detected, starting counter\")\n",
    "                unrecognized_faces[tuple(face_encoding)] = {'counter': 1, 'encoding': face_encoding}\n",
    "\n",
    "            if similar_key is not None and unrecognized_faces[similar_key]['counter'] > unrecognized_threshold:\n",
    "                name = \"Unnamed Person\"\n",
    "                face_encoding_list = unrecognized_faces[similar_key]['encoding'].tolist()\n",
    "                doc_ref = db.collection('people').document()\n",
    "                doc_ref.set({\n",
    "                    'image_enc': face_encoding_list,\n",
    "                    'name': name,\n",
    "                })\n",
    "                face_encodings_from_db, face_names_from_db = fetch_encodings_from_firestore()\n",
    "                del unrecognized_faces[similar_key]\n",
    "            else:\n",
    "                name = \"Processing...\"\n",
    "\n",
    "        face_names.append(name)\n",
    "    \n",
    "    #if at least one face is detected\n",
    "    if(len(face_locations) > 0):\n",
    "        ser.write(str.encode(name + '\\n'))\n",
    "    else:\n",
    "        ser.write(str.encode(\"...\" + '\\n'))\n",
    "\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    cv2.imshow('Video', frame)\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #     break\n",
    "\n",
<<<<<<< Updated upstream
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    res = ser.readline().decode()\n",
    "    # print(res)\n",
    "    if(res == \"on\"):\n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "        print(\"camera is on\")\n",
    "        camera_operations(video_capture, face_encodings_from_db, face_names_from_db)\n",
    "    elif(res == \"off\"):\n",
    "        print(\"camera is off\")\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()"
=======
    "                    name = \"Unnamed Person\"\n",
    "\n",
    "                face_names.append(name)\n",
    "\n",
    "            # Drawing the results on the frame\n",
    "            for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "                top *= 4\n",
    "                right *= 4\n",
    "                bottom *= 4\n",
    "                left *= 4\n",
    "                cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "                cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "                font = cv2.FONT_HERSHEY_DUPLEX\n",
    "                cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "            cv2.imshow('Video', frame)\n",
    "            \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "video_capture = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uploadImageFromPath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m camera_operations(cv2\u001b[39m.\u001b[39;49mVideoCapture(\u001b[39m1\u001b[39;49m))\n",
      "\u001b[1;32m/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X45sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# save the frame into the images folder\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X45sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m cv2\u001b[39m.\u001b[39mimwrite(image_path, face_image)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X45sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m image_url \u001b[39m=\u001b[39m uploadImageFromPath(image_path, image_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X45sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(image_url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X45sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Save the encoding and image URL in Firestore\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'uploadImageFromPath' is not defined"
     ]
    }
   ],
   "source": [
    "camera_operations(cv2.VideoCapture(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install + import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_transcription():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Say something!\")\n",
    "        while True:\n",
    "            audio = recognizer.listen(source)\n",
    "            try:\n",
    "                # Transcribe the audio to text\n",
    "                text = recognizer.recognize_google(audio)\n",
    "                print(\"Transcription: \" + text)\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Could not understand audio\")\n",
    "            except sr.RequestError as e:\n",
    "                print(\"Error; {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Could not find PyAudio; check installation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/speech_recognition/__init__.py:108\u001b[0m, in \u001b[0;36mMicrophone.get_pyaudio\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpyaudio\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyaudio'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m camera_thread\u001b[39m.\u001b[39mstart()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Running audio transcription in the main thread\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m audio_transcription()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Wait for the camera thread to finish\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m camera_thread\u001b[39m.\u001b[39mjoin()\n",
      "\u001b[1;32m/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maudio_transcription\u001b[39m():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     recognizer \u001b[39m=\u001b[39m sr\u001b[39m.\u001b[39mRecognizer()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mwith\u001b[39;00m sr\u001b[39m.\u001b[39;49mMicrophone() \u001b[39mas\u001b[39;00m source:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSay something!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/speech_recognition/__init__.py:80\u001b[0m, in \u001b[0;36mMicrophone.__init__\u001b[0;34m(self, device_index, sample_rate, chunk_size)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(chunk_size, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m chunk_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mChunk size must be a positive integer\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[39m# set up PyAudio\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyaudio_module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_pyaudio()\n\u001b[1;32m     81\u001b[0m audio \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyaudio_module\u001b[39m.\u001b[39mPyAudio()\n\u001b[1;32m     82\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/speech_recognition/__init__.py:110\u001b[0m, in \u001b[0;36mMicrophone.get_pyaudio\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpyaudio\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not find PyAudio; check installation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdistutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m LooseVersion\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m LooseVersion(pyaudio\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m LooseVersion(\u001b[39m\"\u001b[39m\u001b[39m0.2.11\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: Could not find PyAudio; check installation"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/q4/w5wd298x2j51mk7bkv38kmt80000gn/T/ipykernel_11648/1519530862.py\", line 55, in camera_operations\n",
      "cv2.error: Unknown C++ exception from OpenCV code\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Running camera operations in a separate thread\n",
    "camera_thread = threading.Thread(target=camera_operations)\n",
    "camera_thread.start()\n",
    "\n",
    "# Running audio transcription in the main thread\n",
    "audio_transcription()\n",
    "\n",
    "# Wait for the camera thread to finish\n",
    "camera_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAI\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m client \u001b[39m=\u001b[39m OpenAI()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m openai\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msk-GBcT9AD2B4YEzw1se5qrT3BlbkFJzBH7RQACIjcepuVXMUBa\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rachelchen/Documents/GitHub/hackwestern23/test.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m client \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mOpenAI\u001b[39m.\u001b[39mclient()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_client.py:93\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     91\u001b[0m     api_key \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m api_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m api_key\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m organization \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "client = openai.OpenAI.client()\n",
    "\n",
    "audio_file = open(\"speech.mp3\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
